<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta name="google-site-verification" content="ri16BUYqDycq-7QeQFvMtAbUw9lH8AqEeXW0FlCFGN8"> <meta http-equiv="Permissions-Policy" content="interest-cohort=()"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dinh Nam Pham </title> <meta name="author" content="Dinh Nam Pham"> <meta name="description" content="Academic personal page of Dinh Nam Pham. "> <meta name="keywords" content="Dinh Nam Pham, Nam Pham Dinh"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/shock_profile.jpg?1a0ae4be44a5e0cd09719fa3278dfb4b"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://nphamdinh.github.io/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?bf50d6d9dd867d3e0f3b0add94449649"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%64%69%6E%68-%6E%61%6D.%70%68%61%6D%20[%61%74]%20%63%61%6D%70%75%73.%74%75-%62%65%72%6C%69%6E.%64%65" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9431-5614" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=tsm9ycEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Dinh-Pham-11/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/nphamdinh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/dinh-nam-pham-ab8421205" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://dblp.org/pid/377/3274" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/curriculum-vitae/">CV </a> </li> <li class="nav-item "> <a class="nav-link" href="/achievements/">Achievements </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Dinh <span class="font-weight-bold"> </span>Nam Pham </h1> <p class="desc">Aspiring Researcher. CS Student @ <a href="https://www.tu.berlin/en/" rel="external nofollow noopener" target="_blank">TU Berlin</a>. Student Researcher @ <a href="https://www.dfki.de/en/web" rel="external nofollow noopener" target="_blank">DFKI</a>.</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/Nam_headshot-480.webp 480w,/assets/img/Nam_headshot-800.webp 800w,/assets/img/Nam_headshot-1400.webp 1400w," sizes="(min-width: 800px) 231.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/Nam_headshot.jpg?18ec9425b7156335c3d3152541fe4d71" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="Nam_headshot.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p>Hey, I’m Nam, a Machine Learning and Accessible Computing enthusiast! Currently, I’m a computer science student at the <a href="https://www.tu.berlin/en/" rel="external nofollow noopener" target="_blank">Technical University of Berlin</a> and student researcher at the <a href="https://www.dfki.de/en/web" rel="external nofollow noopener" target="_blank">German Research Center for Artificial Intelligence (DFKI)</a>, where I work on AI for recognizing mouth actions in German Sign Language, being advised by Dr. <a href="https://scholar.google.de/citations?view_op=list_works&amp;hl=de&amp;hl=de&amp;user=HhcsbYgAAAAJ&amp;sortby=pubdate" rel="external nofollow noopener" target="_blank">Eleftherios Avramidis</a>. My research interests include applications of machine learning in healthcare, assistive systems and inclusive technologies - which is why I’m currently working on automatic lip reading and sign language.</p> <p>Previously, I’ve interned with <a href="https://www.cares.cam.ac.uk/" rel="external nofollow noopener" target="_blank">Cambridge CARES</a>, a research centre of the University of Cambridge, and <a href="https://www.mercedes-benz-techinnovation.com/en/" rel="external nofollow noopener" target="_blank">Mercedes-Benz Tech Innovation</a>. I was awarded several scholarships such as the <a href="https://www.studienstiftung.de/en/" rel="external nofollow noopener" target="_blank">Studienstiftung</a>, Germany’s most prestigious scholarship, which supports me financially throughout my academic journey.</p> <p>Let’s get in touch! Feel free to contact me for collaborations or opportunities.</p> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="Pham_SLTAT2025png" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/Pham_SLTAT2025.png"><div id="Pham_SLTAT2025png-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('Pham_SLTAT2025png-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="Pham_SLTAT2025png-modal-img"> </div> <script>var modal=document.getElementById("Pham_SLTAT2025png-modal"),img=document.getElementById("Pham_SLTAT2025png"),modalImg=document.getElementById("Pham_SLTAT2025png-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="pham2025sltat" class="col-sm-8 col-md-9"> <div class="title">The Importance of Facial Features in Vision-based Sign Language Recognition: Eyes, Mouth or Full Face?</div> <div class="author"> <em><b>Dinh Nam Pham</b></em>, and Eleftherios Avramidis</div> <div class="periodical"> <em>In ACM International Conference on Intelligent Virtual Agents (IVA Adjunct ’25). 9th International Workshop on Sign Language Translation and Avatar Technology (SLTAT 2025)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.dfki.de/fileadmin/user_upload/import/16012_SLTAT2025_Pham_AAM.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="abstract hidden"> <p>Non-manual facial features play a crucial role in sign language communication, yet their importance in automatic sign language recognition (ASLR) remains underexplored. While prior studies have shown that incorporating facial features can improve recognition, related work often relies on hand-crafted feature extraction and fails to go beyond the comparison of manual features versus the combination of manual and facial features. In this work, we systematically investigate the contribution of distinct facial regions eyes, mouth, and full face using two different deep learning models (a CNN-based model and a transformer-based model) trained on an SLR dataset of isolated signs with randomly selected classes. Through quantitative performance and qualitative saliency map evaluation, we reveal that the mouth is the most important non-manual facial feature, significantly improving accuracy. Our findings highlight the necessity of incorporating facial features in ASLR.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pham2025sltat</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Dinh Nam and Avramidis, Eleftherios}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{The Importance of Facial Features in Vision-based Sign Language  Recognition: Eyes, Mouth or Full Face?}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{ACM International Conference on Intelligent Virtual Agents (IVA  Adjunct ’25). 9th International Workshop on Sign Language Translation and Avatar Technology (SLTAT 2025)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">sep</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{ACM}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{979-8-4007-1996-7/25/09}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{doi.org/10.1145/3742886.3756718}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="Pham_FG2025_thumbnailpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/Pham_FG2025_thumbnail.png"><div id="Pham_FG2025_thumbnailpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('Pham_FG2025_thumbnailpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="Pham_FG2025_thumbnailpng-modal-img"> </div> <script>var modal=document.getElementById("Pham_FG2025_thumbnailpng-modal"),img=document.getElementById("Pham_FG2025_thumbnailpng"),modalImg=document.getElementById("Pham_FG2025_thumbnailpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="pham2025vsrmouthing" class="col-sm-8 col-md-9"> <div class="title">Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language</div> <div class="author"> <em><b>Dinh Nam Pham</b></em>, and Eleftherios Avramidis</div> <div class="periodical"> <em>In 2025 IEEE 19th International Conference on Automatic Face and Gesture Recognition (FG)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.dfki.de/fileadmin/user_upload/import/15873_FG2025_Pham_TransferLearning_VSR_Mouthing.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/FG2025_poster_final.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> <div class="abstract hidden"> <p>Sign Language Recognition (SLR) systems primarily focus on manual gestures, but non-manual features such as mouth movements, specifically mouthing, provide valuable linguistic information. This work directly classifies mouthing instances to their corresponding words in the spoken language while exploring the potential of transfer learning from Visual Speech Recognition (VSR) to mouthing recognition in German Sign Language. We leverage three VSR datasets: one in English, one in German with unrelated words and one in German containing the same target words as the mouthing dataset, to investigate the impact of task similarity in this setting. Our results demonstrate that multi-task learning improves both mouthing recognition and VSR accuracy as well as model robustness, suggesting that mouthing recognition should be treated as a distinct but related task to VSR. This research contributes to the field of SLR by proposing knowledge transfer from VSR to SLR datasets with limited mouthing annotations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">pham2025vsrmouthing</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Dinh Nam and Avramidis, Eleftherios}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 IEEE 19th International Conference on Automatic Face and Gesture Recognition (FG)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Transfer Learning from Visual Speech Recognition to Mouthing Recognition in German Sign Language}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1-6}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/FG61629.2025.11099322}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="disambig_modelpng" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/disambig_model.png"><div id="disambig_modelpng-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('disambig_modelpng-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="disambig_modelpng-modal-img"> </div> <script>var modal=document.getElementById("disambig_modelpng-modal"),img=document.getElementById("disambig_modelpng"),modalImg=document.getElementById("disambig_modelpng-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="phamd2023disambiguating" class="col-sm-8 col-md-9"> <div class="title">Disambiguating Signs: Deep Learning-based Gloss-level Classification for German Sign Language by Utilizing Mouth Actions</div> <div class="author"> <em><b>Dinh Nam Pham</b></em>, Vera Czehmann, and Eleftherios Avramidis</div> <div class="periodical"> <em>In Proceedings of the 31st European Symposium on Artificial Neural Networks, Computational Intelligence, and Machine Learning (ESANN 2023)</em> </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.esann.org/sites/default/files/proceedings/2023/ES2023-168.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/ESANN2023_Disambiguating_Signs_Mouth_Actions.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> <div class="abstract hidden"> <p>Despite the importance of mouth actions in Sign Languages, previous work on Automatic Sign Language Recognition (ASLR) has limited use of the mouth area. Disambiguation of homonyms is one of the functions of mouth actions, making them essential for tasks involving ambiguous hand signs. To measure their importance for ASLR, we trained a classifier to recognize ambiguous hand signs. We compared three models which use the upper body/hands area, the mouth, and both combined as input. We found that the addition of the mouth area in the model resulted in the best accuracy, giving an improvement of 7.2% and 4.7% on the validation and test set, while allowing disambiguation of the hand signs for most of the cases. In cases where the disambiguation failed, it was observed that the signers in the video samples occasionally didn’t perform mouthings. In a few cases, the mouthing was enough to achieve full disambiguation of the signs. We conclude that further investigation on the modelling of the mouth region can be beneficial of future ASLR systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">phamd2023disambiguating</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Disambiguating Signs: Deep Learning-based Gloss-level Classification for German Sign Language by Utilizing Mouth Actions}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Dinh Nam and Czehmann, Vera and Avramidis, Eleftherios}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 31st European Symposium on Artificial Neural Networks, Computational Intelligence, and Machine Learning (ESANN 2023)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{595--600}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{i6doc.com publ.}</span><span class="p">,</span>
  <span class="na">isbn</span> <span class="p">=</span> <span class="s">{978-2-87587-088-9}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.14428/esann/2023.ES2023-168}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 preview"> <img id="beispiel_mouthgif" class="preview z-depth-1 rounded myImg" src="/assets/img/publication_preview/beispiel_mouth.gif"><div id="beispiel_mouthgif-modal" class="modal2"> <span class="closeimg" onclick="document.getElementById('beispiel_mouthgif-modal').style.display='none'">×</span> <div class="myCaption"></div> <img class="modal-content2" id="beispiel_mouthgif-modal-img"> </div> <script>var modal=document.getElementById("beispiel_mouthgif-modal"),img=document.getElementById("beispiel_mouthgif"),modalImg=document.getElementById("beispiel_mouthgif-modal-img"),captionText=modal.getElementsByTagName("div")[0];img.onclick=function(){modal.style.display="block",modalImg.src=this.src,captionText.innerHTML=""};var span=modal.getElementsByTagName("span")[0];span.onclick=function(){modal.style.display="none"},modal.onclick=function(){modal.style.display="none"};</script> </div> <div id="pham2022dev" class="col-sm-8 col-md-9"> <div class="title">Entwicklung und Evaluation eines Deep-Learning-Algorithmus für die Worterkennung aus Lippenbewegungen für die deutsche Sprache</div> <div class="author"> <em><b>Dinh Nam Pham</b></em>, and Torsten Rahne</div> <div class="periodical"> <em>HNO</em>, 2022 </div> <div class="periodical"> ⭐<b>Best Poster Award</b> @ DGA 2022 (German Society of Audiology) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/content/pdf/10.1007/s00106-021-01143-9.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="/assets/pdf/PosterPhamLipreading.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Poster</a> </div> <div class="abstract hidden"> <p>When reading lips, many people benefit from additional visual information from the lip movements of the speaker, which is, however, very error prone. Algorithms for lip reading with artificial intelligence based on artificial neural networks significantly improve word recognition but are not available for the German language. A total of 1806 videoclips with only one German-speaking person each were selected, split into word segments, and assigned to word classes using speech-recognition software. In 38,391 video segments with 32 speakers, 18 polysyllabic, visually distinguishable words were used to train and validate a neural network. The 3D Convolutional Neural Network and Gated Recurrent Units models and a combination of both models (GRUConv) were compared, as were different image sections and color spaces of the videos. The accuracy was determined in 5000 training epochs. Comparison of the color spaces did not reveal any relevant different correct classification rates in the range from 69% to 72%. With a cut to the lips, a significantly higher accuracy of 70% was achieved than when cut to the entire speaker’s face (34%). With the GRUConv model, the maximum accuracies were 87% with known speakers and 63% in the validation with unknown speakers. The neural network for lip reading, which was first developed for the German language, shows a very high level of accuracy, comparable to English-language algorithms. It works with unknown speakers as well and can be generalized with more word classes. </p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">pham2022dev</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Entwicklung und Evaluation eines Deep-Learning-Algorithmus f{\"u}r die Worterkennung aus Lippenbewegungen f{\"u}r die deutsche Sprache}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Pham, Dinh Nam and Rahne, Torsten}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{HNO}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{70}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{6}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{456--465}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s00106-021-01143-9}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%64%69%6E%68-%6E%61%6D.%70%68%61%6D%20[%61%74]%20%63%61%6D%70%75%73.%74%75-%62%65%72%6C%69%6E.%64%65" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://orcid.org/0000-0002-9431-5614" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://scholar.google.com/citations?user=tsm9ycEAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://www.researchgate.net/profile/Dinh-Pham-11/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://github.com/nphamdinh" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/dinh-nam-pham-ab8421205" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://dblp.org/pid/377/3274" title="DBLP" rel="external nofollow noopener" target="_blank"><i class="ai ai-dblp"></i></a> </div> <div class="contact-note">Feel free to contact me via LinkedIn or email (dinh-nam.pham [at] campus.tu-berlin.de) </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Dinh Nam Pham. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?4a129fbf39254905f505c7246e641eaf"></script> <script defer src="/assets/js/copy_code.js?8664456308d8a0b76907c75d01dd1dbf" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>